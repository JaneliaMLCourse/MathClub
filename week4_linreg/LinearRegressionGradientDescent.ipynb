{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=2, floatmode='fixed', suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll solve the system with gradient descent!\n",
    "\n",
    "Check out [this blog post](https://towardsdatascience.com/linear-regression-using-gradient-descent-in-10-lines-of-code-642f995339c0)\n",
    "\n",
    "\n",
    "## Disclaimer!\n",
    "This is for educational purposes.  It *is* reasonable to solve this kind of thing with gradient\n",
    "descent, but there are *lots* of tricks that make it \"stable\".  Don't use gradient descent you've\n",
    "coded yourself in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.array([5,-4,8,-10,1,0]).reshape(-1,1)\n",
    "n2 = np.array([7,-2,7,-9,0,-3]).reshape(-1,1)\n",
    "\n",
    "X = np.concatenate( (np.ones( n1.shape ), n1), axis=1 )\n",
    "y = n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that iteratively solves\n",
    "# the least squares linear regression problem,\n",
    "# By gradient descent\n",
    "\n",
    "# Evil fairies did something nasty to this function,\n",
    "# so that even with the right direction, you may get bad behavior\n",
    "# See if you can figure out what the problem is and how to fix it.\n",
    "#\n",
    "# Debugging hints - \n",
    "# Print out the values of beta at every step.\n",
    "def linear_regression(Xmatrix, yvector, initialBeta=None, maxIterations=1000, stepSize=0.05, convergenceThreshold=0.0001):\n",
    "        # nobs is the number of observations\n",
    "        # nparams is the number of parameters\n",
    "    nobs, nparams = Xmatrix.shape \n",
    "\n",
    "    # X transpose times X\n",
    "    XtX = Xmatrix.T @ Xmatrix\n",
    "\n",
    "    if( initialBeta ):\n",
    "        beta = initialBeta # Use the initializer if we have it\n",
    "    else:\n",
    "        beta = np.zeros((nparams,1)) # Initialize at zero \n",
    "\n",
    "    # TODO - fill in the inital error\n",
    "    lasterror = ?\n",
    "    error = lasterror\n",
    "    \n",
    "    for i in range(maxIterations):\n",
    "\n",
    "        # TODO\n",
    "        # the *negative* of the gradient tells us the direction \n",
    "        # in which to best change the parameters beta\n",
    "        direction = ?\n",
    "\n",
    "        # TODO\n",
    "        # update beta by taking a small step in the direction we chose\n",
    "        beta = ?\n",
    "\n",
    "        # TODO update the error\n",
    "        error = ?\n",
    "        if( error > lasterror ):\n",
    "            print('yikes, the error went up!')\n",
    "            return beta, error\n",
    "        elif( lasterror - error < convergenceThreshold ):\n",
    "            break; # the error isn't changing, we can stop early\n",
    "     \n",
    "        lasterror = error\n",
    "\n",
    "    print( 'ran for %d iterations ' % (i+1) )\n",
    "    \n",
    "    return beta, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta, err = linear_regression( X, y )\n",
    "print( beta )\n",
    "print( err )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_xcoords = np.array([-11,11])\n",
    "lr_ycoords = beta[1] * lr_xcoords + beta[0]\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(n1,n2,s=60)\n",
    "ax.plot( lr_xcoords, lr_ycoords, color='k' )\n",
    "ax.text( -10, 9, 'n2 = %2.2f * n1 + %2.2f'%(beta[1], beta[0]),fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have it working, what answer do we get for our normal-equation breaking example?\n",
    "\n",
    "Xnasty = np.array([[1,0],[1,0]])\n",
    "ynasty = np.array([0,1]).reshape(-1,1)\n",
    "\n",
    "betaNasty, err = linear_regression( Xnasty, ynasty, stepSize=0.01 )\n",
    "print( betaNasty )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
